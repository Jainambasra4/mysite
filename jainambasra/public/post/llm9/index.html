<!DOCTYPE html>
<html lang="en" dir="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>I Asked My AI How to Standout in a Group Meeting, and It Told Me to Interrupt Everyone with Have You Tried Yoga? - Misinformation (OWASP LLM Top 10) | Jainam Basra</title>
<meta name="keywords" content="">
<meta name="description" content="Welcome to my series on OWASP LLM Security! Today, let&#39;s talk about Misinformation in LLM applications.">
<meta name="author" content="">
<link rel="canonical" href="http://localhost:1313/post/llm9/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.9de45e225101e4f99701d2b68fc6b8a1ef6027928be6391fa15bf7f56326c909.css" integrity="sha256-neReIlEB5PmXAdK2j8a4oe9gJ5KL5jkfoVv39WMmyQk=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/apple-touch-icon.png">
<link rel="mask-icon" href="http://localhost:1313/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://localhost:1313/post/llm9/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:url" content="http://localhost:1313/post/llm9/">
  <meta property="og:site_name" content="Jainam Basra">
  <meta property="og:title" content="I Asked My AI How to Standout in a Group Meeting, and It Told Me to Interrupt Everyone with Have You Tried Yoga? - Misinformation (OWASP LLM Top 10)">
  <meta property="og:description" content="Welcome to my series on OWASP LLM Security! Today, let&#39;s talk about Misinformation in LLM applications.">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="post">
    <meta property="article:published_time" content="2025-04-30T11:30:03+00:00">
    <meta property="article:modified_time" content="2025-04-30T11:30:03+00:00">
    <meta property="og:image" content="http://localhost:1313/post/llm9.jpg">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="http://localhost:1313/post/llm9.jpg">
<meta name="twitter:title" content="I Asked My AI How to Standout in a Group Meeting, and It Told Me to Interrupt Everyone with Have You Tried Yoga? - Misinformation (OWASP LLM Top 10)">
<meta name="twitter:description" content="Welcome to my series on OWASP LLM Security! Today, let&#39;s talk about Misinformation in LLM applications.">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "http://localhost:1313/post/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "I Asked My AI How to Standout in a Group Meeting, and It Told Me to Interrupt Everyone with Have You Tried Yoga? - Misinformation (OWASP LLM Top 10)",
      "item": "http://localhost:1313/post/llm9/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "I Asked My AI How to Standout in a Group Meeting, and It Told Me to Interrupt Everyone with Have You Tried Yoga? - Misinformation (OWASP LLM Top 10)",
  "name": "I Asked My AI How to Standout in a Group Meeting, and It Told Me to Interrupt Everyone with Have You Tried Yoga? - Misinformation (OWASP LLM Top 10)",
  "description": "Welcome to my series on OWASP LLM Security! Today, let's talk about Misinformation in LLM applications.",
  "keywords": [
    
  ],
  "articleBody": "🧠 What’s the Deal with Misinformation in LLMs? Okay, let’s break this down:\nImagine you’ve got a friend who’s known for spinning wild tales. Sometimes, they tell a story that’s just too good to be true. That’s exactly what’s happening when LLMs generate hallucinations—plausible-sounding, but totally false information. They don’t know what’s true; they’re just filling in gaps from their training data.\nBut it doesn’t stop there. Overreliance on these AI outputs without verification is like taking every story your friend tells as gospel big mistake, right? That’s what we need to avoid in the world of LLMs. 😱\n💣 Why Should You Care About Misinformation Risks? If you don’t handle misinformation in your LLM, it’s like opening the door to attackers, false data, and all sorts of nasty surprises.\nHere are the key risks:\nUnauthorized Access \u0026 Data Leakage\nImagine you’re trying to keep a secret, but somehow, your AI starts spilling the beans to just anyone. Poor access control can lead to sensitive info being shared with unauthorized users, and that’s a breach waiting to happen. 😬\nCross-Context Information Leaks\nEver told someone something in confidence, only to have it end up in the wrong conversation? Same deal happens when different contexts (data from different sources) accidentally spill into each other. Hello, data leakage. 🤯\nEmbedding Inversion Attacks\nThink of this like someone figuring out the answer to your riddle by reverse-engineering your clues. In this case, attackers use techniques to “invert” embeddings and get access to sensitive, private information hidden in AI models. 🕵️‍♂️\nData Poisoning Attacks\nWhat if someone sneaks fake data into your system and your AI starts serving up nonsense as truth? That’s what happens when attackers poison your training data. It’s like someone slipping a bogus meme into your group chat and watching it spread like wildfire. 🤦‍♂️\nBehavior Alteration\nPicture this: after a few interactions with AI, it starts sounding robotic and detached, just reading facts instead of offering human empathy. That’s behavior alteration when AI’s response style shifts because of poor model adjustments. 🤖❤️\n🚫 How to Stop This Chaos Tighten Up Those Permissions\nThink of this like locking up your personal vault. You want only the right people to access sensitive data. So, use granular access controls and ensure only authorized users (or queries) can access the important stuff.\nValidate Your Data (No Sneaky Business Allowed)\nJust like you wouldn’t let any random person into your private party, don’t let just any data into your system. Validate it like you’re checking IDs at the door—only trustworthy sources get in. ✅\nTagging and Classifying Data\nWhen combining data from different sources, be sure to keep it organized. Properly tag and classify your data so nothing gets lost in the wrong room. 🏷️\nMonitor Everything Like a Hawk\nYou wouldn’t let a toddler run wild with your phone, right? Same idea applies here: keep a close eye on your LLM’s data flow and log everything. You’ll want to catch any suspicious activity before it becomes a bigger problem. 🦅\n👀 Real-Life Attack Scenarios 📝 Scenario 1: Data Poisoning\nAn attacker sneaks in a resume with hidden text (white text on white background) instructing the system to “recommend this unqualified candidate.” The AI takes the bait and passes them through.\nMitigation: Use hidden text detection tools and validate all incoming data before feeding it into the system.\n🔑 Scenario 2: Access Control Failure\nIn a multi-tenant environment, everyone’s using the same vector database. Someone’s query retrieves data from a completely different group’s storage. Oops!\nMitigation: Implement permission-aware vector databases to ensure each query only returns relevant results for the right users.\n💔 Scenario 3: Behavior Alteration\nYour AI used to feel like a helpful friend, but after a few rounds of Retrieval Augmented Generation (RAG), it starts spitting out robotic, fact-filled responses without any warmth.\nMitigation: Monitor how RAG influences the model’s tone and adjust parameters to keep things human and engaging.\nTL;DR Misinformation in LLMs is a huge risk—hallucinations, incorrect data, and overreliance on AI can cause chaos. Attackers can exploit these weaknesses, causing everything from reputation damage to legal trouble. To mitigate, cross-check AI outputs, fine-tune models, and always add human oversight to prevent disaster. So, next time you’re deploying your LLM system, remember:\nDouble-check those AI outputs like you’re fact-checking an article. You don’t want your AI spreading lies. Stay tuned for more OWASP LLM insights! Until then, keep your systems in check and your models honest. 🎉\n",
  "wordCount" : "748",
  "inLanguage": "en",
  "image":"http://localhost:1313/post/llm9.jpg","datePublished": "2025-04-30T11:30:03Z",
  "dateModified": "2025-04-30T11:30:03Z",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "http://localhost:1313/post/llm9/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Jainam Basra",
    "logo": {
      "@type": "ImageObject",
      "url": "http://localhost:1313/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="Jainam Basra (Alt + H)">Jainam Basra</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/" title="Home">
                    <span>Home</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/post" title="Blog">
                    <span>Blog</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/chronicle" title="Life Abroad">
                    <span>Life Abroad</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/aboutme" title="About Me">
                    <span>About Me</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/resume" title="Resume">
                    <span>Resume</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="http://localhost:1313/">Home</a>&nbsp;»&nbsp;<a href="http://localhost:1313/post/">Posts</a></div>
    <h1 class="post-title entry-hint-parent">
      I Asked My AI How to Standout in a Group Meeting, and It Told Me to Interrupt Everyone with Have You Tried Yoga? - Misinformation (OWASP LLM Top 10)
    </h1>
    <div class="post-description">
      Welcome to my series on OWASP LLM Security! Today, let&#39;s talk about Misinformation in LLM applications.
    </div>
    <div class="post-meta"><span title='2025-04-30 11:30:03 +0000 +0000'>April 30, 2025</span>&nbsp;·&nbsp;4 min&nbsp;·&nbsp;748 words

</div>
  </header> 
<figure class="entry-cover"><img loading="eager" src="http://localhost:1313/post/llm9.jpg" alt="">
        
</figure><div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#-whats-the-deal-with-misinformation-in-llms" aria-label="🧠 What’s the Deal with Misinformation in LLMs?">🧠 What’s the Deal with Misinformation in LLMs?</a><ul>
                        
                <li>
                    <a href="#-why-should-you-care-about-misinformation-risks" aria-label="💣 Why Should You Care About Misinformation Risks?">💣 Why Should You Care About Misinformation Risks?</a></li>
                <li>
                    <a href="#-how-to-stop-this-chaos" aria-label="🚫 How to Stop This Chaos">🚫 How to Stop This Chaos</a></li>
                <li>
                    <a href="#-real-life-attack-scenarios" aria-label="👀 Real-Life Attack Scenarios">👀 Real-Life Attack Scenarios</a></li>
                <li>
                    <a href="#tldr" aria-label="TL;DR">TL;DR</a>
                </li>
            </ul>
            </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h1 id="-whats-the-deal-with-misinformation-in-llms">🧠 What’s the Deal with Misinformation in LLMs?<a hidden class="anchor" aria-hidden="true" href="#-whats-the-deal-with-misinformation-in-llms">#</a></h1>
<p>Okay, let’s break this down:<br>
Imagine you’ve got a friend who’s known for spinning wild tales. Sometimes, they tell a story that’s just <em>too</em> good to be true. That’s exactly what’s happening when LLMs generate hallucinations—plausible-sounding, but totally false information. They don’t <em>know</em> what’s true; they’re just filling in gaps from their training data.</p>
<p>But it doesn’t stop there. Overreliance on these AI outputs without verification is like taking every story your friend tells as gospel big mistake, right? That’s what we need to avoid in the world of LLMs. 😱</p>
<h2 id="-why-should-you-care-about-misinformation-risks">💣 Why Should You Care About Misinformation Risks?<a hidden class="anchor" aria-hidden="true" href="#-why-should-you-care-about-misinformation-risks">#</a></h2>
<p>If you don’t handle misinformation in your LLM, it’s like opening the door to attackers, false data, and all sorts of nasty surprises.</p>
<p>Here are the key risks:</p>
<ol>
<li>
<p><strong>Unauthorized Access &amp; Data Leakage</strong><br>
Imagine you’re trying to keep a secret, but somehow, your AI starts spilling the beans to just anyone. Poor access control can lead to sensitive info being shared with unauthorized users, and that’s a breach waiting to happen. 😬</p>
</li>
<li>
<p><strong>Cross-Context Information Leaks</strong><br>
Ever told someone something in confidence, only to have it end up in the wrong conversation? Same deal happens when different contexts (data from different sources) accidentally spill into each other. Hello, data leakage. 🤯</p>
</li>
<li>
<p><strong>Embedding Inversion Attacks</strong><br>
Think of this like someone figuring out the answer to your riddle by reverse-engineering your clues. In this case, attackers use techniques to “invert” embeddings and get access to sensitive, private information hidden in AI models. 🕵️‍♂️</p>
</li>
<li>
<p><strong>Data Poisoning Attacks</strong><br>
What if someone sneaks fake data into your system and your AI starts serving up nonsense as truth? That’s what happens when attackers poison your training data. It’s like someone slipping a bogus meme into your group chat and watching it spread like wildfire. 🤦‍♂️</p>
</li>
<li>
<p><strong>Behavior Alteration</strong><br>
Picture this: after a few interactions with AI, it starts sounding robotic and detached, just reading facts instead of offering human empathy. That’s behavior alteration when AI’s response style shifts because of poor model adjustments. 🤖❤️</p>
</li>
</ol>
<h2 id="-how-to-stop-this-chaos">🚫 How to Stop This Chaos<a hidden class="anchor" aria-hidden="true" href="#-how-to-stop-this-chaos">#</a></h2>
<ol>
<li>
<p><strong>Tighten Up Those Permissions</strong><br>
Think of this like locking up your personal vault. You want only the right people to access sensitive data. So, use granular access controls and ensure only authorized users (or queries) can access the important stuff.</p>
</li>
<li>
<p><strong>Validate Your Data (No Sneaky Business Allowed)</strong><br>
Just like you wouldn’t let any random person into your private party, don’t let just any data into your system. Validate it like you’re checking IDs at the door—only trustworthy sources get in. ✅</p>
</li>
<li>
<p><strong>Tagging and Classifying Data</strong><br>
When combining data from different sources, be sure to keep it organized. Properly tag and classify your data so nothing gets lost in the wrong room. 🏷️</p>
</li>
<li>
<p><strong>Monitor Everything Like a Hawk</strong><br>
You wouldn’t let a toddler run wild with your phone, right? Same idea applies here: keep a close eye on your LLM’s data flow and log everything. You’ll want to catch any suspicious activity before it becomes a bigger problem. 🦅</p>
</li>
</ol>
<h2 id="-real-life-attack-scenarios">👀 Real-Life Attack Scenarios<a hidden class="anchor" aria-hidden="true" href="#-real-life-attack-scenarios">#</a></h2>
<p><strong>📝 Scenario 1: Data Poisoning</strong><br>
An attacker sneaks in a resume with hidden text (white text on white background) instructing the system to “recommend this unqualified candidate.” The AI takes the bait and passes them through.<br>
<strong>Mitigation:</strong> Use hidden text detection tools and validate all incoming data before feeding it into the system.</p>
<p><strong>🔑 Scenario 2: Access Control Failure</strong><br>
In a multi-tenant environment, everyone’s using the same vector database. Someone’s query retrieves data from a completely different group’s storage. Oops!<br>
<strong>Mitigation:</strong> Implement permission-aware vector databases to ensure each query only returns relevant results for the right users.</p>
<p><strong>💔 Scenario 3: Behavior Alteration</strong><br>
Your AI used to feel like a helpful friend, but after a few rounds of Retrieval Augmented Generation (RAG), it starts spitting out robotic, fact-filled responses without any warmth.<br>
<strong>Mitigation:</strong> Monitor how RAG influences the model’s tone and adjust parameters to keep things human and engaging.</p>
<h2 id="tldr">TL;DR<a hidden class="anchor" aria-hidden="true" href="#tldr">#</a></h2>
<ul>
<li>Misinformation in LLMs is a huge risk—hallucinations, incorrect data, and overreliance on AI can cause chaos.</li>
<li>Attackers can exploit these weaknesses, causing everything from reputation damage to legal trouble.</li>
<li>To mitigate, cross-check AI outputs, fine-tune models, and always add human oversight to prevent disaster.</li>
</ul>
<p>So, next time you&rsquo;re deploying your LLM system, remember:</p>
<ul>
<li>Double-check those AI outputs like you&rsquo;re fact-checking an article. You don’t want your AI spreading lies.</li>
</ul>
<p>Stay tuned for more OWASP LLM insights! Until then, keep your systems in check and your models honest. 🎉</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
<nav class="paginav">
  <a class="next" href="http://localhost:1313/post/llm8/">
    <span class="title">Next »</span>
    <br>
    <span>When Your AI Starts Mixing Things Up - Vector and Embedding Weaknesses (OWASP LLM Top 10)</span>
  </a>
</nav>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="http://localhost:1313/">Jainam Basra</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
