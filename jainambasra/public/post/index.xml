<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Posts on Jainam Basra</title>
    <link>http://localhost:1313/post/</link>
    <description>Recent content in Posts on Jainam Basra</description>
    <generator>Hugo -- 0.143.1</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 18 Feb 2025 11:30:03 +0000</lastBuildDate>
    <atom:link href="http://localhost:1313/post/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>‚ò†Ô∏è Data Poisoning in AI: How Your Model Might Be a Sleeper Agent! (OWASP LLM Top 10)</title>
      <link>http://localhost:1313/post/llm4/</link>
      <pubDate>Tue, 18 Feb 2025 11:30:03 +0000</pubDate>
      <guid>http://localhost:1313/post/llm4/</guid>
      <description>Welcome to my series on OWASP LLM Security! Today, let&amp;#39;s talk about Data &amp;amp; Model Poisoning where attackers manipulate training data, making AI models biased, toxic, or even a secret weapon waiting to be triggered.</description>
    </item>
    <item>
      <title>LLM03:2025 LLM Supply Chain: Who‚Äôs Messing with My AI Ingredients? (OWASP LLM Top 10)</title>
      <link>http://localhost:1313/post/llm3/</link>
      <pubDate>Tue, 11 Feb 2025 11:30:03 +0000</pubDate>
      <guid>http://localhost:1313/post/llm3/</guid>
      <description>Supply chains aren‚Äôt just for logistics anymore LLMs have their own, and they‚Äôre just as vulnerable. Let‚Äôs talk about how attackers can poison your models, sneak malware into your adapters, and turn your AI masterpiece into a security nightmare.</description>
    </item>
    <item>
      <title>LLM02:2025 - Sensitive Information Disclosure (OWASP LLM Top 10)</title>
      <link>http://localhost:1313/post/llm2/</link>
      <pubDate>Sun, 09 Feb 2025 11:30:03 +0000</pubDate>
      <guid>http://localhost:1313/post/llm2/</guid>
      <description>Welcome to my new series on AI LLM Red Teaming! Today, let&amp;#39;s Understand the Risks and Mitigations with Real-Life Fun Analogies.</description>
    </item>
    <item>
      <title>üî¥ Hacking AI: Tricking the Model into Revealing Secrets! The Art of Prompt Injection (OWASP LLM Top 10)</title>
      <link>http://localhost:1313/post/llm1/</link>
      <pubDate>Fri, 07 Feb 2025 11:30:03 +0000</pubDate>
      <guid>http://localhost:1313/post/llm1/</guid>
      <description>Welcome to my new series on AI LLM Red Teaming! Today, let&amp;#39;s talk about Prompt Injection‚Äîthe cybersecurity nightmare that makes LLMs spill secrets, break rules, and sometimes, even gaslight you.</description>
    </item>
  </channel>
</rss>
